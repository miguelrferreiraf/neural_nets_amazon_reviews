{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1rHUyxTNypsgOMjmh6dNp0zXLEagWfrq5",
      "authorship_tag": "ABX9TyOmLnV0CKEB6PMTvUw6MOpw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguelrferreiraf/neural_nets_amazon_reviews/blob/master/sentiment_analysis_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__author__ = \"Miguel R. Ferreira Filho\"\n",
        "__email__ = \"miguel.ribeiro@live.com\"\n",
        "__website__ = \"miguelrferreiraf.github.io\""
      ],
      "metadata": {
        "id": "QqWJg9tMkHru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANALYSING AMAZON REVIEWS WITH LSTM NEURAL NETS\n",
        "\n",
        "\"Your artificial intelligences are as smart as you\" (Me, motherfucker! 1998-)\n",
        "\n",
        "![alt text](https://images.unsplash.com/photo-1506880018603-83d5b814b5a6?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8cmVhZGluZ3xlbnwwfHwwfHw%3D&w=1000&q=80)\n",
        "\n",
        "Hello, folks!\n",
        "\n",
        "Now I am presenting unto you a very very interesting project! \n",
        "\n",
        "It is small on size but theoretically vast and complex, all of it in a short code! You will be introduced to some really deep concepts on machine learning, like LSTM (long-short term memory) neural archtecture for classificaion models, tokenizers and word embedding algorithms.\n",
        "\n",
        "Resumely, that`s th deepest that deep learning can dive in terms of simluating a human inteligence!\n",
        "\n",
        "An artificial inteligence that can read!\n",
        "\n",
        "This project uses some interesting technologies allied with neural nets to make a very interesting thing: analisying a text (a movie review) and get to know if it is a good, regular or bad review. One of these technologies is the tokenizer, whose function and theoretical explantion will be extended down here. We also are going to use the embedding technology in our LSTM neural netowrk construction. \n",
        "\n",
        "We may extend our explanation a little bit to linguistic field but, as I use to say in all my projects, we must not dive deeply in this subject since I'm programmer, not a Noam Chomsky (although Chomsky had made nice contributions to programming languages building. And although he wouldn't restrain himself of lecturing me in my field). \n",
        "\n",
        "We are going talk about the LSTM format for neural networks too. That's basicaly a archtecture format for neural nets. "
      ],
      "metadata": {
        "id": "40GjhqVjM68c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importation\n",
        "\n",
        "The first step of it is, of course, importing our libraries. The special thing here is the tokenizer! The rest is more of the same, those are libraries we can find in a plenty of projects of neural nets. "
      ],
      "metadata": {
        "id": "puSSFHjFN4m7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQrp_x1IDwiz"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import keras\n",
        "from keras import Model\n",
        "from tensorflow.keras.layers import Flatten,LSTM,Dense,Flatten,Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from keras.initializers import glorot_uniform\n",
        "from sklearn import model_selection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data extraction\n",
        "\n",
        "Here it goes our data. It's a csv archive, my favorite format for working. In our data, we are using litterally 3.6 million reviews of movies from Amazon website. It's 656 MB data. Since I'm working at Colab, I've mounted it in order to taking it from a different folder. Check it out:   "
      ],
      "metadata": {
        "id": "1VjdlNcoQIj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read in data\n",
        "with open('/content/drive/MyDrive/CoÃÅdigos/Data_Scienc_Projects_(from-books)/sentiment_analysis-amazon-reviews(FINALIZADO)/data/train.csv', 'r') as file:\n",
        "  text = file.readlines()\n"
      ],
      "metadata": {
        "id": "PBMJBHfTg7p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are going to create a dataframe to store all the data that will be used by our our model:"
      ],
      "metadata": {
        "id": "vDOsXvZLTQtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create empty dataframe\n",
        "x_train = pd.DataFrame()"
      ],
      "metadata": {
        "id": "LIGvAdZ-hRUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see. it's empty.\n",
        "\n",
        "Now, we fill our dataframe with the data from the csv archive. \n",
        "\n",
        "We must turn the words of the csv list into strings for it may be usable. There's noo tokenization without that.\n",
        "\n",
        "We can use a trick to make a loop by using `for` command. The loop may find out the words and convert it to strings automatically. Then we're gonna see the data."
      ],
      "metadata": {
        "id": "pW_cMUaFTf55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fill in dataframe\n",
        "word=[]\n",
        "label=[]\n",
        "for n in text:\n",
        "  n=n.split()\n",
        "  label.append(1) if n[0] ==\"__label__2\" else label.append(0)\n",
        "  word.append(\" \".join(n[1:]))\n",
        "x_train['consumer_review'] = word\n",
        "x_train['polarity_label'] = label\n",
        "\n",
        "#view dataframe\n",
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "b6UAcotjjUd6",
        "outputId": "1fca7a8e-aee2-4495-b4ae-bf157694c6b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           consumer_review  polarity_label\n",
              "0        even for the non-gamer\",\"This sound track was ...               0\n",
              "1        best soundtrack ever to anything.\",\"I'm readin...               0\n",
              "2        soundtrack is my favorite music of all time, h...               0\n",
              "3        Soundtrack\",\"I truly like this soundtrack and ...               0\n",
              "4        Pull Your Jaw Off The Floor After Hearing it\",...               0\n",
              "...                                                    ...             ...\n",
              "3599995  do it!!\",\"The high chair looks great when it f...               0\n",
              "3599996  nice, low functionality\",\"I have used this hig...               0\n",
              "3599997  but hard to clean\",\"We have a small house, and...               0\n",
              "3599998  is it saying?\",\"not sure what this book is sup...               0\n",
              "3599999  My Blood Run Red-White-And-Blue\",\"I agree that...               0\n",
              "\n",
              "[3600000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3cc0e870-8e42-4dd5-b6dd-a5b65a6ce505\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>consumer_review</th>\n",
              "      <th>polarity_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>even for the non-gamer\",\"This sound track was ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>best soundtrack ever to anything.\",\"I'm readin...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>soundtrack is my favorite music of all time, h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Soundtrack\",\"I truly like this soundtrack and ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Pull Your Jaw Off The Floor After Hearing it\",...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3599995</th>\n",
              "      <td>do it!!\",\"The high chair looks great when it f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3599996</th>\n",
              "      <td>nice, low functionality\",\"I have used this hig...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3599997</th>\n",
              "      <td>but hard to clean\",\"We have a small house, and...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3599998</th>\n",
              "      <td>is it saying?\",\"not sure what this book is sup...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3599999</th>\n",
              "      <td>My Blood Run Red-White-And-Blue\",\"I agree that...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3600000 rows √ó 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3cc0e870-8e42-4dd5-b6dd-a5b65a6ce505')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3cc0e870-8e42-4dd5-b6dd-a5b65a6ce505 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3cc0e870-8e42-4dd5-b6dd-a5b65a6ce505');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data cleaning and splitting\n",
        "\n",
        "This project uses basic step-by-step of an IA project. Above, you will see how this one is build beggining from here, the cleaning and spliting of the data:\n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=13F7yfl28_CmD_5eamJ1cdUIlbJNVoaeG)\n",
        "\n",
        "As almost all IA project, it has a specific part for data loading and data preparation. Here we prepare the data using the basic line of command `train_test_split`. We're going to separate 20% of the data for training the model.\n",
        "\n",
        "Here we use only 20% of data to avoid overloading your system. You can reduce or increase this number according to your convenience.\n",
        "\n"
      ],
      "metadata": {
        "id": "0dhGw1OkUN9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_, x_set,_, y_set = model_selection.train_test_split(x_train['consumer_review'], \n",
        "                                                     x_train['polarity_label'],\n",
        "                                                     test_size=0.02)\n",
        "                                "
      ],
      "metadata": {
        "id": "DJ7AMs-UmTvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We must turn the data readable for the computer. So, we must remove capitalizations and ponctuations that may disturb computer comprehension.\n",
        "\n",
        "Attention!: this code lines right bellow can be extremely useful for your next text reading neural network algorithm. I'd pay  attention on it.\n",
        "\n",
        "The tokenizer work can be extemely simplified through the application of this simple trick:"
      ],
      "metadata": {
        "id": "RQsWldd4XvTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data cleaning function\n",
        "def data_prep(in_tex):\n",
        "  # Remove punctuations and numbers\n",
        "  out_tex = re.sub('[^a-zA-Z]', ' ', in_tex)\n",
        "  # Convert upper case to lower case\n",
        "  out_tex=\"\".join(list(map(lambda x:x.lower(),out_tex)))\n",
        "  # Remove single character\n",
        "  out_tex= re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', out_tex)\n",
        "  return out_tex"
      ],
      "metadata": {
        "id": "_FMpSBLuoKPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create a new dataframe list to store our newly cleaned data:"
      ],
      "metadata": {
        "id": "Km8EAfW_YoLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create new list with clean data\n",
        "text_set=[]\n",
        "for reviews in list(x_set):\n",
        "  text_set.append(data_prep(reviews))"
      ],
      "metadata": {
        "id": "osZ3vGNFoNiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train= pd.DataFrame()\n",
        "x_train['consumer_review'] = text_set\n",
        "x_train['polarity_label'] = list(y_set)"
      ],
      "metadata": {
        "id": "w93OMMb1oTCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The splitting\n",
        "\n",
        "It has come to the traditional 70:30 ratio splitting of data training and data testing. It goes as the previous time we splited part of the data as workable. Then, we apply the tokenizer. "
      ],
      "metadata": {
        "id": "CWTkiVZyZAl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split data into 70% train and 30% test\n",
        "x_train, x_test, y_train, y_test = \\\n",
        "model_selection.train_test_split(x_train['consumer_review'],\n",
        "x_train['polarity_label'],\n",
        "\n",
        "test_size=0.30)"
      ],
      "metadata": {
        "id": "eNshy0EzoVFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The tokenizer\n",
        "\n",
        "Sentence tokenization or text segmentation is the process of splitting the structure of the text into sentences that act as the first level of tokens the text body is contained of. The objective is turning this tokens into meaningful sentences.\n",
        "\n",
        "Tokenization can be described as the process of breaking down a text into smaller parts with the minimal amount of semantic sense.\n",
        "\n",
        "Each token comprehend a small textual component that hold the minimal amount of semantic and definite syntax. \n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1BcoSF-MKVyxR8MGn3VYOx8l4giXbOOSH)\n",
        "\n",
        "(How many tokens for sentence there are in a King James Bible? See above)\n",
        "\n",
        "Before we apply the tokenizer, we need to convert our list into a array list. That's how it works:"
      ],
      "metadata": {
        "id": "NrluvpAOZvHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to array\n",
        "x_train=np.array(x_train.values.tolist())\n",
        "x_test=np.array(x_test.values.tolist())\n",
        "y_train=np.array(y_train.values.tolist())\n",
        "y_test=np.array(y_test.values.tolist())\n"
      ],
      "metadata": {
        "id": "A3f9ZPMWoZDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying tokenizer\n",
        "\n",
        "See how many tokens we've got from the data."
      ],
      "metadata": {
        "id": "kTrdRv1gd7v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "word_index=tokenizer.word_index\n",
        "total_size = len(word_index)+1\n",
        "print(total_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSoUGCk-odLS",
        "outputId": "dfe36c4d-4148-446e-c609-ea571b6129ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''In order to make the model processing easier, we convert the text to input format.'''\n",
        "\n",
        "#text to sequence\n",
        "x_train = tokenizer.texts_to_sequences(x_train)\n",
        "x_test = tokenizer.texts_to_sequences(x_test)"
      ],
      "metadata": {
        "id": "v8nYd89dokEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add padding to ensure the same length\n",
        "max_length = 100\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=max_length)\n",
        "x_test = pad_sequences(x_test, padding='post', maxlen=max_length)"
      ],
      "metadata": {
        "id": "O_1WMNVgomh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation, LSTM and Embedding\n",
        "\n",
        "### LSTM\n",
        "\n",
        "LSTM is a traditional neural network archtecture which keeps a set of hidden states variables that results in a different form of the neural net map over time. It has, resumely, three type of gates:  \n",
        "\n",
        "* forget gate (Œ±) which inform the cell what information to forget by multiplying the matrix resulted from the functions by zero to forget the information (or one, to keep it).\n",
        "* input gate (z) that inform what information to store in the cell through and sigmoid activation function. It multiplies the previously obtained data by a value [0,1] - between 0 and 1 but never equals to any of them. \n",
        "\n",
        " This cell will have also to discard some information so there is an input modulation gate with a tanh activation function, which range values are [-1, 1].\n",
        "* output or reset gate (r) that decide what information goes to next cell.\n",
        "\n",
        "We may also assume a cell gate (c), that receive or gives the resgressive memory (h) - in case r receives no memory from Œ± - from the previous or next LSTM cell, depending on time (t). \n",
        "\n",
        "If you remember our Wine Quality Prediction Machine Learning Project, we can remember this: \n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1hgaTGG8TwX2KcwE__xecnMFWzjfwI3wl)\n",
        "\n",
        "This is a RNN sigle neuron all by himself with an inner linear unit function. LSTM neurons with all their four layers looks like this: \n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1Tl50n9xTCfNFM95-jNq1kieaMFQkbJon)\n",
        "\n",
        "where U is an extra parameter of the standard sigmoid function.\n",
        "\n",
        "Now this is pure mathematics almost devoided of visual explanation, so we are not going to dive in deeply. Let's make a resume: \n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1aI5IkW571dD9L20PjWIO5sn2_PWKkDWT)\n",
        "\n",
        "The first standard RNN contains a cell with a single layer with a single tanh activation function. LSTM are quite similar except it has 4 layers in the cell with four activatioon functions. Inside the LSTM cell, we can see the four layers, activation functions and hidden states through time (t) described above:\n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1QdHpLSnVa5mHtHG_97jS86cBLBK9-ziQ)\n",
        "\n",
        "I¬¥d go further but the books I'm using represents the equations and math differently of each other so Id have to translate to avoid mathematical incoherence. Let's just move on.\n",
        "\n",
        "### Embedding cell\n",
        "\n",
        "For writing this project I use a book called \"Some shit\", by Muthafoocka, PhD, which whose chapter about embeddings is called \"Why are we crazy for embeddings?\". \n",
        "\n",
        "You're going to understand it right now.\n",
        "\n",
        "What`s a embedding? Well, resumely: \n",
        "\n",
        "An embedding is a fixed-length vector typically used to encode and represent an entity as document, sentence, word or graph. \n",
        "\n",
        "In our context, a vector means that the LSTM model is capable to bring data from differents \"sorrounding places\" arround the central data. \n",
        "\n",
        "When you want to train your neural network for, let's say, identify an bird specie, you have a way to turn image (of the bird) into numbers - through pixelation, for example. When identifying a music gender, you have an mathematically precise way to turn sound into images (through spectograms) and, consequently, into numbers. But all this data is \"dense\". They're immediatly correlative.\n",
        "\n",
        "But how can you turn a word into a number? Words are sparse. A word may noot be relative to next one from itself.\n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1iBkpDkeajjZnOuMDdwz-FCFrmRQgVq1A)\n",
        "\n",
        "There comes the embedding!\n",
        "\n",
        "What the embedding does is to 'stretch' a vector from a primal token (remember them?) to the tokens around it throughout a text to establish a semantic coherence.\n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1P8Dooifw07SsxFl49IG78MjB1q5mTTrC)\n",
        "\n",
        "Let's pretend we want to procede embedding for a small string like `['movie', 'was', 'good']`. We can assume tha the embedding will pick a number for each word, like, 1, 2 and 3. For vast arrays of tokens, the number will extend far bigger, becoming a whole vocablary, usually  called `vocab_map`.\n",
        "\n",
        "For a previous `vocab_map`, our embedding assumed that the numbe for each token is 2, 57 and 121. By the image bellow, we assume our `vocab_map` has 82358 tokens.\n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1AcimTazqMZs3Wa3K13Xe84zye95dBeIB)\n",
        "\n",
        "\n",
        "\n",
        "1.   In the first paramater, we see the respective number of each token at the `vocab_map` and see `vocab_size` (82358).\n",
        "2.   Then we see the dimension of the dense embedding: 128.\n",
        "3.   Finally, we see the length of the input sequences.\n",
        "\n",
        "\n",
        "For this review, we have a dimension of [1,3]. The numeric sequnece is [2, 57, 121]. Now, the columns of the indices in the review sequence are selected from the embedding layer to generate the final word embedding.\n",
        "\n",
        "This gives us a embedding vector of (1, 3, 128) or (1, 128, 3). There's no difference at last two number positions transposed (you may try both on the embedding and see by yourself).\n",
        "\n",
        "After this, the embedding layers are finally passed to te embedding layer, which gives us the following network format:\n",
        "\n",
        "![ALT TEXT](http://drive.google.com/uc?id=1DlzmRXEpLVgt_cCbaGIbloRqWwY4oQBf)\n",
        "\n",
        "Now, let's finally create our neural network using the LSTM layer, the Keras embedding layer and a dense layer:\n",
        "\n"
      ],
      "metadata": {
        "id": "eAAJColIgu49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_size, 20, input_length=max_length))\n",
        "model.add(LSTM(32,dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "k0E9Agubos08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compilation"
      ],
      "metadata": {
        "id": "FSrxddWtxDFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#compile\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x99917sbovOj",
        "outputId": "07c6aab2-9097-47c9-e203-fad5e90b91a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 20)           1528700   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                6784      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,535,517\n",
            "Trainable params: 1,535,517\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fitting\n",
        "\n",
        "Let's fit our model."
      ],
      "metadata": {
        "id": "ULV6eZH1xNeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=128, epochs=5,\n",
        "verbose=1, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozOQ-f-LoyAz",
        "outputId": "9fd030fa-f124-4b30-8dfe-c10349b0a7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "394/394 [==============================] - 59s 142ms/step - loss: 0.0416 - acc: 0.9965 - val_loss: 4.1182e-04 - val_acc: 1.0000\n",
            "Epoch 2/5\n",
            "394/394 [==============================] - 56s 141ms/step - loss: 2.7682e-04 - acc: 1.0000 - val_loss: 1.7298e-04 - val_acc: 1.0000\n",
            "Epoch 3/5\n",
            "394/394 [==============================] - 56s 141ms/step - loss: 1.3527e-04 - acc: 1.0000 - val_loss: 9.7186e-05 - val_acc: 1.0000\n",
            "Epoch 4/5\n",
            "394/394 [==============================] - 56s 142ms/step - loss: 8.1011e-05 - acc: 1.0000 - val_loss: 6.2114e-05 - val_acc: 1.0000\n",
            "Epoch 5/5\n",
            "394/394 [==============================] - 56s 142ms/step - loss: 5.3745e-05 - acc: 1.0000 - val_loss: 4.2673e-05 - val_acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7fec0ba710>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "ruNgiheYxXZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "x2KQVoC7o5pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model reloading\n",
        "\n",
        "This code line above can e used to reload the pretrained model. Surely you need to train it harder for a better result. For a better training, we can add more layers at our LSTM model or make them denser."
      ],
      "metadata": {
        "id": "6RFJZxR_xkh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model(\"model.h5\")\n"
      ],
      "metadata": {
        "id": "249zWQMgo7uD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c452a497-6b76-4179-b5d7-edfc3d69fa45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "As you can see, this is a project simple in practice and very extensive on theory. blablablablabla... \n",
        "\n",
        "Bye bye!\n",
        "\n",
        "![alt text](https://c.tenor.com/pyGugHcm4XcAAAAC/michael-scott-the-office.gif)\n",
        "\n",
        "By: Miguel Ferreira."
      ],
      "metadata": {
        "id": "TAcRlxL51Qbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! jupyter nbconvert --to html sentiment_analysis_reviews.ipynb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf12Ei2Ceols",
        "outputId": "580dd39d-6011-49b5-d1cf-3b2cffad102b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook sentiment_analysis_reviews.ipynb to html\n",
            "[NbConvertApp] Writing 317488 bytes to sentiment_analysis_reviews.html\n"
          ]
        }
      ]
    }
  ]
}